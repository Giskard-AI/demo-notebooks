{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sarcasm classification [tensorflow]\n",
    "* Binary classification of tweets being sarcastic or normal.\n",
    "* Reference notebook: <https://www.kaggle.com/code/madz2000/sarcasm-detection-with-glove-word2vec-83-accuracy/notebook#Introduction-to-GloVe>\n",
    "* Dataset: <https://www.kaggle.com/code/madz2000/sarcasm-detection-with-glove-word2vec-83-accuracy/input>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install necessary libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install nltk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true,
    "nbpresent": {
     "id": "543fa1c3-ebfd-47c4-bdc7-558a9aa7d38f"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import string\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.utils import pad_sequences\n",
    "from keras.preprocessing import text\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional\n",
    "\n",
    "import giskard\n",
    "from giskard import Dataset, Model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Constants.\n",
    "TEXT_COLUMN_NAME = \"headline\"\n",
    "TARGET_COLUMN_NAME = \"is_sarcastic\"\n",
    "\n",
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "MAX_TOKENS = 25000\n",
    "EMBEDDING_DIM = 200\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "\n",
    "# Paths.\n",
    "DATA_DIR = os.path.join(\".\", \"datasets\", \"sarcasm_classification_dataset\")\n",
    "EMBEDDING_FILE = os.path.join(DATA_DIR, \"glove_200d.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "nbpresent": {
     "id": "f903a8cf-34b8-44eb-89b7-00333d8c400d"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "nbpresent": {
     "id": "65e910a4-7e2a-480a-8974-12e823efa5a9"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(**kwargs) -> pd.DataFrame:\n",
    "    \"\"\"Load data.\"\"\"\n",
    "    print(f\"Loading data...\")\n",
    "\n",
    "    df = pd.read_json(os.path.join(DATA_DIR, \"Sarcasm_Headlines_Dataset_v2.json\"), lines=True, **kwargs)\n",
    "    df = df.drop(columns=\"article_link\")\n",
    "\n",
    "    print(f\"Finished loading data! \\nShape: {df.shape} \\nColumns: {df.columns.tolist()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "sarcasm_df = load_data(nrows=2000)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "nbpresent": {
     "id": "dd2f5d03-0e72-44da-85ef-8de7a534608e"
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Define text preprocessing logic"
   ]
  },
  {
   "metadata": {
    "trusted": true,
    "nbpresent": {
     "id": "d9cafddf-c417-461f-872b-af66ce94d3fc"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Perform text-data cleaning: punctuation and stop words removal.\"\"\"\n",
    "\n",
    "    # Remove punctuation.\n",
    "    df[TEXT_COLUMN_NAME] = df[TEXT_COLUMN_NAME].apply(\n",
    "        lambda sentence: sentence.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "    # Remove stop words.\n",
    "    df[TEXT_COLUMN_NAME] = df[TEXT_COLUMN_NAME].apply(\n",
    "        lambda sentence: ' '.join([_word for _word in sentence.split() if _word.lower() not in STOPWORDS]))\n",
    "\n",
    "    return df[TEXT_COLUMN_NAME]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train-test split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(sarcasm_df[[TEXT_COLUMN_NAME]], sarcasm_df[TARGET_COLUMN_NAME], random_state=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrap data with giskard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_data = pd.concat([X_test.copy(), Y_test.copy()], axis=1)\n",
    "wrapped_data = Dataset(raw_data, name=\"sarcasm\", target=TARGET_COLUMN_NAME, column_types={TEXT_COLUMN_NAME: \"text\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "trusted": true,
    "nbpresent": {
     "id": "0b3c8f6d-1f96-4630-b907-558949668c89"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Cleaning train data...\")\n",
    "X_train = clean_text(X_train)\n",
    "\n",
    "print(f\"Cleaning test data...\")\n",
    "X_test = clean_text(X_test)\n",
    "\n",
    "print(f\"Finished cleaning!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "nbpresent": {
     "id": "ea4e8cb5-06b7-4dc1-8ce6-e1f5ca1b686f"
    }
   },
   "cell_type": "code",
   "source": [
    "# Fit tokenizer.\n",
    "tokenizer = text.Tokenizer(num_words=MAX_TOKENS)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Define the number of words' embeddings to store. +1, cause tokens indexing starts with 1.\n",
    "num_tokens = min(MAX_TOKENS, len(tokenizer.word_index)) + 1\n",
    "\n",
    "# Tokenize train text.\n",
    "tokenized_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(tokenized_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Tokenize test text.\n",
    "tokenized_test = tokenizer.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(tokenized_test, maxlen=MAX_SEQUENCE_LENGTH)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define preprocessing function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocessing_function(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Preprocessing function used by giskard.\"\"\"\n",
    "    # Clean text.\n",
    "    cleaned_text = clean_text(df)\n",
    "\n",
    "    # Tokenize text.\n",
    "    tokens = tokenizer.texts_to_sequences(cleaned_text)\n",
    "    tokens_with_padding = pad_sequences(tokens, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    return tokens_with_padding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create embeddings matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "trusted": true,
    "nbpresent": {
     "id": "88b74dd6-f700-4f29-9ccf-3debd1769c10"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_line(word: str, *arr: list) -> Tuple[str, np.ndarray]:\n",
    "    \"\"\"Parse line from the file with embeddings.\n",
    "    The first value of the line is the word and the rest values are related glove embedding: (<word>, 0.66, 0.23, ...).\"\"\"\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def init_embeddings_matrix(embeddings_dict: dict) -> np.ndarray:\n",
    "    \"\"\"Initialization of the matrix, where each row is a specific embedding vector.\"\"\"\n",
    "    stacked_embeddings = np.stack(list(embeddings_dict.values()))\n",
    "    embeddings_mean, embeddings_std, embeddings_dimension = stacked_embeddings.mean(), stacked_embeddings.std(), stacked_embeddings.shape[1]\n",
    "    embeddings_matrix = np.random.normal(embeddings_mean, embeddings_std, (num_tokens, embeddings_dimension))\n",
    "\n",
    "    return embeddings_matrix\n",
    "\n",
    "def get_embeddings_matrix() -> np.ndarray:\n",
    "    \"\"\"Create matrix, where each row is an embedding of a specific word.\"\"\"\n",
    "    print(f\"Building embeddings matrix...\")\n",
    "\n",
    "    # Load glove embeddings.\n",
    "    embeddings_dict = dict(parse_line(*line.rstrip().rsplit(' ')) for line in open(EMBEDDING_FILE))\n",
    "\n",
    "    # Initialization of embeddings matrix.\n",
    "    embeddings_matrix = init_embeddings_matrix(embeddings_dict)\n",
    "\n",
    "    # Fill-in embeddings matrix with glove word vectors.\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if idx >= num_tokens:\n",
    "            continue\n",
    "\n",
    "        embedding_vector = embeddings_dict.get(word, None)\n",
    "\n",
    "        if embedding_vector is not None:\n",
    "            embeddings_matrix[idx] = embedding_vector\n",
    "\n",
    "    print(f\"Finished building embedding matrix!\")\n",
    "\n",
    "    return embeddings_matrix\n",
    "\n",
    "embed_matrix = get_embeddings_matrix()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_model(weights) -> Sequential:\n",
    "    # Define model container.\n",
    "    _model = Sequential()\n",
    "\n",
    "    # Embedding layer.\n",
    "    _model.add(Embedding(num_tokens, output_dim=EMBEDDING_DIM, weights=[weights], input_length=EMBEDDING_DIM, trainable=True))\n",
    "\n",
    "    # LSTM stage.\n",
    "    _model.add(Bidirectional(LSTM(units=128 , recurrent_dropout=0.5, dropout=0.5)))\n",
    "\n",
    "    # Dense stage.\n",
    "    _model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Build model.\n",
    "    _model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['acc'])\n",
    "    return _model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {
    "trusted": true,
    "nbpresent": {
     "id": "f373f09a-ed45-4070-b065-38312d9c9c5a"
    }
   },
   "cell_type": "code",
   "source": [
    "model = init_model(embed_matrix)\n",
    "model.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "nbpresent": {
     "id": "54cae72a-4d47-4dfc-92f3-47f9c5fd0504"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define hyperparameters.\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "# Fit model.\n",
    "_ = model.fit(X_train, Y_train, batch_size=batch_size , validation_data=(X_test, Y_test), epochs=epochs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true,
    "nbpresent": {
     "id": "da893fca-2589-443a-8f65-99c9866407c3"
    }
   },
   "cell_type": "code",
   "source": [
    "train_metric = model.evaluate(X_train, Y_train)[1]\n",
    "test_metric = model.evaluate(X_test, Y_test)[1]\n",
    "print(f\"Train accuracy: {train_metric}\\n\"\n",
    "      f\"Test accuracy: {test_metric}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrap model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wrapped_model = Model(model,\n",
    "                      model_type=\"classification\",\n",
    "                      data_preprocessing_function=preprocessing_function,\n",
    "                      name=\"sarcasm_classification\",\n",
    "                      feature_names=[TEXT_COLUMN_NAME],\n",
    "                      classification_threshold=0.5,\n",
    "                      classification_labels=[1, 0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scan model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wrapped_model.predict(wrapped_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scanning_result = giskard.scan(wrapped_model, wrapped_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(scanning_result)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
