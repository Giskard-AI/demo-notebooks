{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SMS spam classification [sklearn]\n",
    "Binary classification of sms being spam or not.\n",
    "Reference: <https://www.kaggle.com/code/faressayah/natural-language-processing-nlp-for-beginners>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install necessary packages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install nltk giskard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Iterable\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "import giskard\n",
    "from giskard import wrap_dataset, wrap_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download NLTK corpus"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Notebook-level settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Necessary for custom preprocessing function transformers.\n",
    "sklearn.set_config(transform_output=\"pandas\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Constants.\n",
    "TARGET_NAME = \"label\"\n",
    "TEXT_COLUMN_NAME = \"message\"\n",
    "\n",
    "# Paths.\n",
    "DATA_DIRECTORY = os.path.join(\".\", \"datasets\", \"sms_spam_classification_dataset\", \"spam.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and initially preprocess data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29b534c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T22:12:56.201446Z",
     "iopub.status.busy": "2023-02-16T22:12:56.200671Z",
     "iopub.status.idle": "2023-02-16T22:12:56.283323Z",
     "shell.execute_reply": "2023-02-16T22:12:56.281887Z"
    },
    "papermill": {
     "duration": 0.101748,
     "end_time": "2023-02-16T22:12:56.285847",
     "exception": false,
     "start_time": "2023-02-16T22:12:56.184099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data() -> pd.DataFrame:\n",
    "    \"\"\"Load data.\"\"\"\n",
    "    df = pd.read_csv(DATA_DIRECTORY, encoding='latin-1')\n",
    "    df.dropna(how=\"any\", inplace=True, axis=1)\n",
    "    df.columns = [TARGET_NAME, TEXT_COLUMN_NAME]\n",
    "    return df\n",
    "\n",
    "messages_df = load_data()\n",
    "messages_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_label(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Change string labels to the integer encoding.\"\"\"\n",
    "    df.label = LabelEncoder().fit_transform(df.label)\n",
    "    return df\n",
    "\n",
    "messages_df = preprocess_label(messages_df)\n",
    "messages_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train-test split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b33667",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T22:12:59.658981Z",
     "iopub.status.busy": "2023-02-16T22:12:59.658525Z",
     "iopub.status.idle": "2023-02-16T22:12:59.670509Z",
     "shell.execute_reply": "2023-02-16T22:12:59.669471Z"
    },
    "papermill": {
     "duration": 0.03499,
     "end_time": "2023-02-16T22:12:59.673230",
     "exception": false,
     "start_time": "2023-02-16T22:12:59.638240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(messages_df[[TEXT_COLUMN_NAME]], messages_df[TARGET_NAME], random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define preprocessing pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_punctuation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove punctuation from text.\"\"\"\n",
    "    df[TEXT_COLUMN_NAME] = df[TEXT_COLUMN_NAME].apply(lambda text: text.translate(str.maketrans('', '', string.punctuation)))\n",
    "    return df\n",
    "\n",
    "remove_punctuation_transformer = FunctionTransformer(remove_punctuation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_stop_words(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove stopwords from text.\"\"\"\n",
    "    _STOPWORDS = stopwords.words('english') + ['u', 'Ã¼', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n",
    "\n",
    "    df[TEXT_COLUMN_NAME] = df[TEXT_COLUMN_NAME].apply(lambda text: ' '.join([word for word in text.split() if word.lower() not in _STOPWORDS]))\n",
    "    return df\n",
    "\n",
    "remove_stop_words_transformer = FunctionTransformer(remove_stop_words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adapt_vectorizer_input(df: pd.DataFrame) -> Iterable:\n",
    "    \"\"\"Adapt input for the vectorizers.\n",
    "\n",
    "    The problem is that vectorizers accept iterable, not DataFrame, but Series. Thus, we need to ravel dataframe with text have input single dimension.\n",
    "    Issue reference: https://stackoverflow.com/questions/50665240/valueerror-found-input-variables-with-inconsistent-numbers-of-samples-1-3185\"\"\"\n",
    "\n",
    "    df = df.iloc[:, 0]\n",
    "    return df\n",
    "\n",
    "adapt_vectorizer_input_transformer = FunctionTransformer(adapt_vectorizer_input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Naive Bayes model pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bac208",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T22:13:00.512698Z",
     "iopub.status.busy": "2023-02-16T22:13:00.512267Z",
     "iopub.status.idle": "2023-02-16T22:13:00.612825Z",
     "shell.execute_reply": "2023-02-16T22:13:00.611460Z"
    },
    "papermill": {
     "duration": 0.125448,
     "end_time": "2023-02-16T22:13:00.615922",
     "exception": false,
     "start_time": "2023-02-16T22:13:00.490474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define data preprocessor pipeline.\n",
    "preprocessor = Pipeline(steps=[\n",
    "    (\"punctuation_remover\", remove_punctuation_transformer),\n",
    "    (\"stop_words_remover\", remove_stop_words_transformer),\n",
    "    (\"text_vectorizer_adapter\", adapt_vectorizer_input_transformer),\n",
    "    ('bow', CountVectorizer()),\n",
    "    ('tfid', TfidfTransformer()),\n",
    "])\n",
    "\n",
    "# Define general pipeline with data preprocessing and model.\n",
    "pipeline_naive_bayes = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    ('model', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Fit model.\n",
    "pipeline_naive_bayes.fit(X_train, y_train)\n",
    "y_pred_prob = pipeline_naive_bayes.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Get test metric.\n",
    "metric = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "print(f\"Test ROC-AUC score: {metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e816488",
   "metadata": {
    "papermill": {
     "duration": 0.019578,
     "end_time": "2023-02-16T22:13:00.655711",
     "exception": false,
     "start_time": "2023-02-16T22:13:00.636133",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Logistic Regression model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define general pipeline with data preprocessing and model.\n",
    "pipeline_logistic_regression = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),\n",
    "    (\"logistic_regression\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit model.\n",
    "pipeline_logistic_regression.fit(X_train, y_train)\n",
    "y_pred_prob = pipeline_logistic_regression.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Get test metric.\n",
    "metric = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "print(f\"Test ROC-AUC score: {metric}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrap data and models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Wrap test dataset.\n",
    "raw_dataset = pd.concat([X_test, y_test], axis=1)\n",
    "wrapped_dataset = wrap_dataset(raw_dataset,\n",
    "                               name=\"sms_spam\",\n",
    "                               target=\"label\",\n",
    "                               column_types={\"message\": \"text\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Wrap Naive-Bayes model.\n",
    "wrapped_model_naive_bayes = wrap_model(pipeline_naive_bayes,\n",
    "                                       model_type=\"classification\",\n",
    "                                       name=\"spam_classifier_naive_bayes\",\n",
    "                                       feature_names=X_test.columns,\n",
    "                                       classification_threshold=0.5)\n",
    "wrapped_model_naive_bayes.predict(wrapped_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Wrap Logistic Regression model.\n",
    "wrapped_model_logistic_regression = wrap_model(pipeline_logistic_regression,\n",
    "                                               model_type=\"classification\",\n",
    "                                               name=\"spam_classifier_logistic_regression\",\n",
    "                                               feature_names=X_test.columns,\n",
    "                                               classification_threshold=0.5)\n",
    "wrapped_model_logistic_regression.predict(wrapped_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scan models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scan Naive Bayes model.\n",
    "naive_bayes_scan = giskard.scan(wrapped_model_naive_bayes, wrapped_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(naive_bayes_scan)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scan Logistic regression model.\n",
    "logistic_regression_scan = giskard.scan(wrapped_model_logistic_regression, wrapped_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(logistic_regression_scan)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18.393171,
   "end_time": "2023-02-16T22:13:02.202180",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-16T22:12:43.809009",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
