{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M5 Sales prediction [sklearn, lgbm]\n",
    "Prediction of the products' demand for the next 28 days.\n",
    "Reference: <https://www.kaggle.com/code/ragnar123/very-fst-model>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgbm\n",
    "from sklearn import preprocessing, metrics\n",
    "\n",
    "import giskard\n",
    "from giskard import wrap_dataset, wrap_model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Notebook settings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define constants"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Constants.\n",
    "ID_COLUMN = \"id\"\n",
    "TARGET_COLUMN = \"demand\"\n",
    "SPLIT_DATE = \"2016-03-27\"\n",
    "\n",
    "# Paths.\n",
    "DATA_DIR = os.path.join(\".\", \"datasets\", \"m5_sales_prediction\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and preprocess data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_data(n_series_use=100):\n",
    "    \"\"\"Load necessary data files.\"\"\"\n",
    "    print('Loading data...')\n",
    "\n",
    "    # Calendar data.\n",
    "    _calendar_df = pd.read_csv(os.path.join(DATA_DIR, \"calendar.csv\"))\n",
    "    print(f'Calendar has {_calendar_df.shape[0]} rows and {_calendar_df.shape[1]} columns')\n",
    "\n",
    "    # Prices data.\n",
    "    _prices_df = pd.read_csv(os.path.join(DATA_DIR, 'sell_prices.csv'))\n",
    "    print(f'Sell prices has {_prices_df.shape[0]} rows and {_prices_df.shape[1]} columns')\n",
    "\n",
    "    # Sales data.\n",
    "    _sales_df = pd.read_csv(os.path.join(DATA_DIR, 'sales_train_validation.csv'))\n",
    "    _sales_df = _sales_df.iloc[:n_series_use]\n",
    "    print(f'Sales train validation has {_sales_df.shape[0]} rows and {_sales_df.shape[1]} columns')\n",
    "\n",
    "    print(\"Data is loaded!\")\n",
    "\n",
    "    return _calendar_df, _prices_df, _sales_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_data(_calendar_df, _prices_df, _sales_df):\n",
    "    \"\"\"Preprocess and create df with the whole data.\"\"\"\n",
    "    print(\"Preprocessing data...\")\n",
    "\n",
    "    # Melt the sales data: translate columnar demand representation into single target vector.\n",
    "    _data = pd.melt(_sales_df,\n",
    "                    id_vars=[ID_COLUMN, 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],\n",
    "                    var_name='day', value_name=TARGET_COLUMN)\n",
    "\n",
    "    # Add the calendar data.\n",
    "    _calendar_df.drop(['weekday', 'wday', 'month', 'year'], inplace=True, axis=1)\n",
    "    _data = pd.merge(_data, _calendar_df, how ='left', left_on=['day'], right_on=['d'])\n",
    "    _data.drop(['d', 'day'], inplace=True, axis=1)\n",
    "\n",
    "    # Add the sell price data.\n",
    "    _data = _data.merge(_prices_df, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "\n",
    "    #TODO: Push below steps into preprocessing function of wrap_model.\n",
    "    # Fill NaN values.\n",
    "    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in nan_features:\n",
    "        _data[feature].fillna('unknown', inplace=True)\n",
    "\n",
    "    # Encode categorical features.\n",
    "    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    for feature in cat:\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        _data[feature] = encoder.fit_transform(_data[feature])\n",
    "\n",
    "    print(f'Final dataset has {_data.shape[0]} rows and {_data.shape[1]} columns')\n",
    "    print(\"Data preprocessed!\")\n",
    "\n",
    "    return _data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = preprocess_data(*load_data())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Enrich data with rolling features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_features(_data):\n",
    "    \"\"\"Add new features.\"\"\"\n",
    "    # Add rolling demand features. For each record features are calculated in the time-window of range [shift + rolling; shift].\n",
    "    print(\"Producing rolling features...\")\n",
    "    _data['lag_t28'] = _data.groupby([ID_COLUMN])[TARGET_COLUMN].transform(lambda x: x.shift(28))\n",
    "    _data['lag_t29'] = _data.groupby([ID_COLUMN])[TARGET_COLUMN].transform(lambda x: x.shift(29))\n",
    "    _data['lag_t30'] = _data.groupby([ID_COLUMN])[TARGET_COLUMN].transform(lambda x: x.shift(30))\n",
    "\n",
    "    _data['rolling_mean_t7'] = _data.groupby([ID_COLUMN])[TARGET_COLUMN].transform(lambda x: x.shift(28).rolling(7).mean())\n",
    "    _data['rolling_std_t7'] = _data.groupby([ID_COLUMN])[TARGET_COLUMN].transform(lambda x: x.shift(28).rolling(7).std())\n",
    "\n",
    "    _data['rolling_mean_t30'] = _data.groupby([ID_COLUMN])[TARGET_COLUMN].transform(lambda x: x.shift(28).rolling(30).mean())\n",
    "    _data['rolling_std_t30'] = _data.groupby([ID_COLUMN])[TARGET_COLUMN].transform(lambda x: x.shift(28).rolling(30).std())\n",
    "    _data['rolling_skew_t30'] = _data.groupby([ID_COLUMN])[TARGET_COLUMN].transform(lambda x: x.shift(28).rolling(30).skew())\n",
    "    _data['rolling_kurt_t30'] = _data.groupby([ID_COLUMN])[TARGET_COLUMN].transform(lambda x: x.shift(28).rolling(30).kurt())\n",
    "\n",
    "    _data['rolling_mean_t90'] = _data.groupby([ID_COLUMN])[TARGET_COLUMN].transform(lambda x: x.shift(28).rolling(90).mean())\n",
    "    _data['rolling_mean_t180'] = _data.groupby([ID_COLUMN])[TARGET_COLUMN].transform(lambda x: x.shift(28).rolling(180).mean())\n",
    "\n",
    "    # Add price features.\n",
    "    print(\"Producing price features...\")\n",
    "    _data['lag_price_t1'] = _data.groupby([ID_COLUMN])['sell_price'].transform(lambda x: x.shift(1))\n",
    "    _data['price_change_t1'] = (_data['lag_price_t1'] - _data['sell_price']) / (_data['lag_price_t1'])\n",
    "\n",
    "    _data['rolling_price_max_t365'] = _data.groupby([ID_COLUMN])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max())\n",
    "    _data['price_change_t365'] = (_data['rolling_price_max_t365'] - _data['sell_price']) / (_data['rolling_price_max_t365'])\n",
    "\n",
    "    _data['rolling_price_std_t7'] = _data.groupby([ID_COLUMN])['sell_price'].transform(lambda x: x.rolling(7).std())\n",
    "    _data['rolling_price_std_t30'] = _data.groupby([ID_COLUMN])['sell_price'].transform(lambda x: x.rolling(30).std())\n",
    "\n",
    "    _data.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1)\n",
    "\n",
    "    # Add time features.\n",
    "    print(\"Producing time features...\")\n",
    "    _data['date'] = pd.to_datetime(_data['date'])\n",
    "    _data['year'] = _data['date'].dt.year\n",
    "    _data['month'] = _data['date'].dt.month\n",
    "    _data['week'] = _data['date'].dt.week\n",
    "    _data['day'] = _data['date'].dt.day\n",
    "    _data['dayofweek'] = _data['date'].dt.dayofweek\n",
    "\n",
    "    print(\"Features added!\")\n",
    "\n",
    "    return _data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = add_features(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train-validation split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_val_split(_data):\n",
    "    \"\"\"Perform train/val split, where the split point is the date '2016-03-27'. Validation records are 28 days for each product.\"\"\"\n",
    "\n",
    "    print(\"Splitting data...\")\n",
    "    # Train data.\n",
    "    x_train = _data[_data['date'] <= SPLIT_DATE]\n",
    "    y_train = x_train[TARGET_COLUMN]\n",
    "\n",
    "    # Validation data.\n",
    "    x_val = _data[_data['date'] > SPLIT_DATE]\n",
    "    y_val = x_val[TARGET_COLUMN]\n",
    "\n",
    "    # Drop date feature.\n",
    "    x_train.drop(\"date\", inplace=True, axis=1)\n",
    "    x_val.drop(\"date\", inplace=True, axis=1)\n",
    "\n",
    "    print(\"Data was split!\")\n",
    "    print(f\"Train samples: {len(x_train)}\\n\"\n",
    "          f\"Valid samples: {len(x_val)}\")\n",
    "\n",
    "    return x_train, y_train, x_val, y_val"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val = train_val_split(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filter necessary features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def filter_features(_x):\n",
    "    _FEATURES_USE = [\n",
    "        'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'year', 'month', 'week', 'day', 'dayofweek',\n",
    "        'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'sell_price',\n",
    "        'lag_t28', 'lag_t29', 'lag_t30', 'rolling_mean_t7', 'rolling_std_t7', 'rolling_mean_t30', 'rolling_mean_t90',\n",
    "        'rolling_mean_t180', 'rolling_std_t30', 'price_change_t1', 'price_change_t365', 'rolling_price_std_t7',\n",
    "        'rolling_price_std_t30', 'rolling_skew_t30', 'rolling_kurt_t30'\n",
    "    ]\n",
    "    return _x[_FEATURES_USE]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = filter_features(X_train)\n",
    "X_val = filter_features(X_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build and fit estimator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_model(x_train, y_train, x_val, y_val):\n",
    "    _ESTIMATOR_PARAMS = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'metric': 'rmse',\n",
    "        'objective': 'regression',\n",
    "        'n_jobs': -1,\n",
    "        'seed': 236,\n",
    "        'learning_rate': 0.1,\n",
    "        'bagging_fraction': 0.75,\n",
    "        'bagging_freq': 10,\n",
    "        'colsample_bytree': 0.75,\n",
    "        'n_estimators': 200\n",
    "    }\n",
    "\n",
    "    # Fit estimator.\n",
    "    print(f\"Model training...\")\n",
    "    estimator = lgbm.LGBMRegressor(**_ESTIMATOR_PARAMS)\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    # Validate estimator.\n",
    "    print(f\"Model validation...\")\n",
    "    val_pred = estimator.predict(x_val)\n",
    "    val_score = np.sqrt(metrics.mean_squared_error(val_pred, y_val))\n",
    "    print(f'Validation RMSE-score: {val_score}')\n",
    "\n",
    "    return estimator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = build_model(X_train, Y_train, X_val, Y_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Wrap data and estimator"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Wrap dataset.\n",
    "categoricals = X_val.select_dtypes(int).columns.tolist()\n",
    "raw_dataset = pd.concat([X_val, Y_val], axis=1)\n",
    "wrapped_dataset = wrap_dataset(raw_dataset,\n",
    "                               name=\"m5_products_timeseries_dataset\",\n",
    "                               target=TARGET_COLUMN,\n",
    "                               cat_columns=categoricals)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Wrap model.\n",
    "wrapped_model = wrap_model(model,\n",
    "                           model_type=\"regression\",\n",
    "                           name=\"m5_timeseries_regressor\",\n",
    "                           feature_names=X_val.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Scan model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scanning_results = giskard.scan(wrapped_model, wrapped_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display(scanning_results)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
